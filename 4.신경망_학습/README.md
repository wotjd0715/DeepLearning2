이번장은 신경망을 학습해보자   
학습이란 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻합니다.   
신경망이 학습할 수 있도록 해주는 지표인 손실함수를 소개

이 손실함수의 결과값을 최소화 하는 가중치를 찾는것이 학습

## 손실함수

1. 오차제곱합   
y = [0.2,0.3,0.5] #예측값을 softmax로 표현   
t = [0,0,1] #정답레이블을 원-핫 인코딩으로 표현   
0.5*sum( (y-t)^2 )으로 하여 오차율을 구함

2.교차엔트로피 오차   
 -sum( t* log(y +delta ))

## 미니배치 학습
손실함수를 구하는건 학습데이터 1개당 하나의 손실함수를 구한다.   
만약 학습데이터가 100장일때 1epoch마다 100개의 손실함수를 구해 평균을 낸다음 평균손실함수를
구한다. 이렇게 할경우 학습을 진행하는데 있어 시간이 많이 소요된다.   
따라서 100장중 10장만 무작위로 뽑아가며 10장만으로 학습을 진행하는 것을 미니배치 학습이라고 한다.

## 경사 하강법
손실함수를 최소화 시키는 최적의 매개변수(가중치,편향)을 찾기위해서 우리는 경사 하강법을 사용합니다.   
이때 우리는 미분을 이용하게 됩니다. 어떤 함수의 최솟값으로 가는 방법은 그 함수의 미분값(접선의 기울기)에따라
이동하면됩니다. 손실함수를 f, 매개변수를 x라고 할 경우 df/dx가 양수가 되면 x를 감소하는 방향으로 음수가 되면 x를 증가시키는 방향으로 x를 바꾸어줍니다.   
이때 바꾸어 주는 정도는 x = x - n(df/dx)이며 여기서 n은 learning_rate를 뜻합니다.   
추가로 여기서 f의 매개변수가 a,b,..로 여러개라면 (df/da, df/db,...)처럼 모든 변수의 편미분을 벡터로 나타낸것을 `기울기`라고 합니다.

## 신경망에서의 기울기 
가중치 W가 2x3의 행령과 손실함수 L을 생각해보면 dL/dW역시 2x3 행렬의 형태를 띄게 됩니다.   
또한 여기서 dL/dW의 2x3행렬의 원소의 값에 따라 손실함수의 감소에 영향을 주는 정도도 달라진다.

## 학습 알고리즘
1단계:미니배치 -> 2단계: 기울기 산출 -> 3단계: 매개변수 갱신   

위 과정을 계속 반복하며 손실함수의 최대값을 찾는 방법을 학습이라하며 더 자세히는 미니배치 때문에 무작위로 골라내기
때문에 `확률적 경사 하강법`이라 한다.